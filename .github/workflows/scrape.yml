name: Scrape Senego & SeneNews Daily

on:
  schedule:
    - cron: '0 6 * * *'  # Tous les jours à 6h UTC (7h au Sénégal)
  workflow_dispatch:     # Permet un déclenchement manuel
    inputs:
      days_back:
        description: 'Nombre de jours à scraper'
        required: false
        default: '1'
        type: string
      max_pages:
        description: 'Nombre maximum de pages par site'
        required: false
        default: '15'
        type: string

jobs:
  scrape-and-push:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Limite de temps pour éviter les blocages
    
    permissions:
      contents: write
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Check existing data
        id: check_existing
        run: |
          if [ -f "articles_scraped.csv" ]; then
            article_count=$(python -c "import pandas as pd; print(len(pd.read_csv('articles_scraped.csv')))" 2>/dev/null || echo "0")
            echo "existing_articles=$article_count" >> $GITHUB_OUTPUT
            echo "📊 Articles existants: $article_count"
          else
            echo "existing_articles=0" >> $GITHUB_OUTPUT
            echo "📄 Aucun fichier existant trouvé"
          fi
          
      - name: Run unified scraping script
        id: scraping
        run: |
          echo "🚀 Démarrage du scraping..."
          
          # Paramètres configurables
          DAYS_BACK="${{ github.event.inputs.days_back || '1' }}"
          MAX_PAGES="${{ github.event.inputs.max_pages || '15' }}"
          
          echo "📅 Scraping des $DAYS_BACK derniers jours"
          echo "📄 Maximum $MAX_PAGES pages par site"
          
          # Modifier le script pour accepter les paramètres
          python -c "
          import sys
          import os
          
          # Ajouter le répertoire courant au path
          sys.path.insert(0, '.')
          
          # Importer et lancer le scraper
          from scraper import UnifiedNewsScraper
          
          scraper = UnifiedNewsScraper()
          success = scraper.scrape_all(days_back=int('$DAYS_BACK'), max_pages=int('$MAX_PAGES'))
          
          if success:
              print('✅ Scraping réussi avec nouvelles données')
              exit(0)
          else:
              print('ℹ️ Scraping terminé sans nouvelles données')
              exit(1)
          " 2>&1 | tee scraping_log.txt
          
          # Capturer le code de sortie
          SCRAPING_EXIT_CODE=${PIPESTATUS[0]}
          echo "scraping_success=$([[ $SCRAPING_EXIT_CODE -eq 0 ]] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT
          
          # Afficher le log pour debug
          echo "=== LOG DE SCRAPING ==="
          cat scraping_log.txt
          
      - name: Analyze scraped data
        if: always()
        id: analyze_data
        run: |
          if [ -f "articles_scraped.csv" ]; then
            echo "📊 Analyse des données..."
            
            python -c "
            import pandas as pd
            from datetime import datetime, timedelta
            
            try:
                df = pd.read_csv('articles_scraped.csv')
                total_articles = len(df)
                print(f'📈 Total articles: {total_articles}')
                
                # Analyser les articles récents (dernières 24h)
                if 'date_parsed' in df.columns:
                    df['date_parsed'] = pd.to_datetime(df['date_parsed'], errors='coerce')
                    yesterday = datetime.now() - timedelta(days=1)
                    recent_articles = df[df['date_parsed'] >= yesterday]
                    print(f'🆕 Articles récents (24h): {len(recent_articles)}')
                    
                    if len(recent_articles) > 0:
                        print('📊 Nouveaux articles par source:')
                        source_counts = recent_articles['source'].value_counts()
                        for source, count in source_counts.items():
                            print(f'   • {source}: {count} articles')
                        
                        print('🏷️ Nouveaux articles par thème:')
                        theme_counts = recent_articles['theme'].value_counts().head(5)
                        for theme, count in theme_counts.items():
                            print(f'   • {theme}: {count} articles')
                
                # Vérifier s'il y a eu des changements
                with open('$GITHUB_OUTPUT', 'a') as f:
                    f.write(f'total_articles={total_articles}\n')
                    if 'date_parsed' in df.columns and len(recent_articles) > 0:
                        f.write(f'has_new_data=true\n')
                        f.write(f'new_articles_count={len(recent_articles)}\n')
                    else:
                        f.write(f'has_new_data=false\n')
                        f.write(f'new_articles_count=0\n')
                        
            except Exception as e:
                print(f'❌ Erreur analyse: {e}')
                with open('$GITHUB_OUTPUT', 'a') as f:
                    f.write('has_new_data=false\n')
                    f.write('total_articles=0\n')
                    f.write('new_articles_count=0\n')
            "
          else
            echo "❌ Aucun fichier de données trouvé"
            echo "has_new_data=false" >> $GITHUB_OUTPUT
            echo "total_articles=0" >> $GITHUB_OUTPUT
            echo "new_articles_count=0" >> $GITHUB_OUTPUT
          fi
          
      - name: Commit and push changes
        if: steps.analyze_data.outputs.has_new_data == 'true'
        run: |
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Ajouter tous les fichiers CSV modifiés
          git add articles_scraped.csv
          
          # Vérifier s'il y a des changements à commiter
          if ! git diff --cached --quiet; then
            # Créer un message de commit informatif
            TOTAL_ARTICLES="${{ steps.analyze_data.outputs.total_articles }}"
            NEW_ARTICLES="${{ steps.analyze_data.outputs.new_articles_count }}"
            TIMESTAMP=$(date +'%Y-%m-%d %H:%M UTC')
            
            git commit -m "📰 Daily scraping update - ${NEW_ARTICLES} nouveaux articles (${TOTAL_ARTICLES} total) - ${TIMESTAMP}"
            git push origin main
            
            echo "✅ Données mises à jour et poussées avec succès"
            echo "📊 $NEW_ARTICLES nouveaux articles ajoutés"
            echo "📈 $TOTAL_ARTICLES articles au total"
          else
            echo "ℹ️ Aucun changement détecté dans les fichiers"
          fi
          
      - name: Generate scraping report
        if: always()
        run: |
          echo "=== 📊 RAPPORT DE SCRAPING ===" >> scraping_report.md
          echo "**Date:** $(date +'%Y-%m-%d %H:%M UTC')" >> scraping_report.md
          echo "**Statut:** $([[ '${{ steps.scraping.outputs.scraping_success }}' == 'true' ]] && echo '✅ Succès' || echo '⚠️ Aucune nouvelle donnée')" >> scraping_report.md
          echo "" >> scraping_report.md
          
          if [ -f "articles_scraped.csv" ]; then
            echo "**📈 Statistiques:**" >> scraping_report.md
            echo "- Total articles: ${{ steps.analyze_data.outputs.total_articles }}" >> scraping_report.md
            echo "- Nouveaux articles: ${{ steps.analyze_data.outputs.new_articles_count }}" >> scraping_report.md
            echo "" >> scraping_report.md
            
            # Ajouter le top des thèmes
            echo "**🏷️ Top 5 des thèmes:**" >> scraping_report.md
            python -c "
            import pandas as pd
            try:
                df = pd.read_csv('articles_scraped.csv')
                if 'theme' in df.columns:
                    theme_counts = df['theme'].value_counts().head(5)
                    for i, (theme, count) in enumerate(theme_counts.items(), 1):
                        print(f'{i}. {theme}: {count} articles')
                else:
                    print('Données de thème non disponibles')
            except:
                print('Erreur lors de l analyse des thèmes')
            " >> scraping_report.md
          else
            echo "**❌ Aucune donnée disponible**" >> scraping_report.md
          fi
          
          echo "" >> scraping_report.md
          echo "---" >> scraping_report.md
          
          # Afficher le rapport
          echo "=== 📋 RAPPORT FINAL ==="
          cat scraping_report.md
          
      - name: Upload scraping logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: scraping-logs-${{ github.run_number }}
          path: |
            scraping_log.txt
            scraping_report.md
          retention-days: 7
          
      - name: Cleanup temporary files
        if: always()
        run: |
          # Nettoyer les fichiers temporaires
          rm -f scraping_log.txt scraping_report.md
          
          # Garder seulement le fichier principal
          find . -name "articles_unifies_*.csv" -delete 2>/dev/null || true
          
          echo "🧹 Nettoyage terminé"
name: Scrape Senego & SeneNews Daily

on:
  schedule:
    - cron: '0 0 * * *'  # Tous les jours √† minuit UTC
  workflow_dispatch:     # Permet un d√©clenchement manuel

jobs:
  scrape-and-push:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Run unified scraping script
        run: |
          python scraper.py
          
      - name: Check for new data
        id: check_changes
        run: |
          if ls articles_unifies_*.csv 1> /dev/null 2>&1; then
            echo "new_data=true" >> $GITHUB_OUTPUT
            echo "csv_file=$(ls articles_unifies_*.csv | head -1)" >> $GITHUB_OUTPUT
          else
            echo "new_data=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Merge with existing data
        if: steps.check_changes.outputs.new_data == 'true'
        run: |
          python -c "
          import pandas as pd
          import os
          from datetime import datetime
          
          # Nom du nouveau fichier
          new_file = '${{ steps.check_changes.outputs.csv_file }}'
          main_file = 'articles_scraped.csv'
          
          # Charger les nouvelles donn√©es
          new_data = pd.read_csv(new_file)
          print(f'Nouvelles donn√©es: {len(new_data)} articles')
          
          # V√©rifier si le fichier principal existe
          if os.path.exists(main_file):
              existing_data = pd.read_csv(main_file)
              print(f'Donn√©es existantes: {len(existing_data)} articles')
              
              # Combiner et d√©dupliquer par URL
              combined_data = pd.concat([existing_data, new_data], ignore_index=True)
              combined_data = combined_data.drop_duplicates(subset=['url'], keep='last')
              print(f'Donn√©es combin√©es apr√®s d√©duplication: {len(combined_data)} articles')
          else:
              combined_data = new_data
              print('Cr√©ation du fichier principal avec les nouvelles donn√©es')
          
          # Trier par date si possible
          if 'date_parsed' in combined_data.columns:
              combined_data = combined_data.sort_values('date_parsed', ascending=False)
          
          # Sauvegarder
          combined_data.to_csv(main_file, index=False, encoding='utf-8')
          print(f'Fichier principal mis √† jour: {main_file}')
          
          # Supprimer le fichier temporaire
          os.remove(new_file)
          "
          
      - name: Commit and push changes
        if: steps.check_changes.outputs.new_data == 'true'
        run: |
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Ajouter les fichiers modifi√©s
          git add articles_scraped.csv
          
          # V√©rifier s'il y a des changements √† commiter
          if ! git diff --cached --quiet; then
            # Compter les articles dans le fichier
            article_count=$(python -c "import pandas as pd; print(len(pd.read_csv('articles_scraped.csv')))")
            git commit -m "üì∞ Daily scraping update - ${article_count} articles total - $(date +'%Y-%m-%d %H:%M UTC')"
            git push origin master
            echo "‚úÖ Data successfully updated and pushed"
          else
            echo "‚ÑπÔ∏è No new articles found today"
          fi
          
      - name: Scraping summary
        if: always()
        run: |
          echo "=== SCRAPING SUMMARY ==="
          if [ -f "articles_scraped.csv" ]; then
            total_articles=$(python -c "import pandas as pd; print(len(pd.read_csv('articles_scraped.csv')))" 2>/dev/null || echo "0")
            echo "üìä Total articles in database: $total_articles"
            
            echo "üìà Recent themes distribution:"
            python -c "
            import pandas as pd
            try:
                df = pd.read_csv('articles_scraped.csv')
                if 'theme' in df.columns:
                    theme_counts = df['theme'].value_counts().head(5)
                    for theme, count in theme_counts.items():
                        print(f'   ‚Ä¢ {theme}: {count} articles')
                else:
                    print('   No theme data available')
            except Exception as e:
                print(f'   Error reading data: {e}')
            " 2>/dev/null || echo "   Could not analyze themes"
          else
            echo "‚ùå No data file found"
          fi
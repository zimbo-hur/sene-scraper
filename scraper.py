#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import pandas as pd
import csv
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import time
import sys

class UnifiedNewsScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.all_articles = []
    
    def harmonize_theme(self, original_theme):
        """Harmonise les th√®mes selon les r√®gles sp√©cifi√©es"""
        if not original_theme or pd.isna(original_theme):
            return "Autre"
        
        theme = str(original_theme).strip()
        theme_lower = theme.lower()
        
        # R√®gles de mapping sp√©cifiques
        theme_mapping = {
            # Politique
            'politique': 'Politique',
            
            # Economie
            'economie': 'Economie',
            '√©conomie': 'Economie',
            'p√™che': 'Economie',
            'peche': 'Economie',
            
            # Sport
            'sport': 'Sport',
            'sports': 'Sport',
            'football': 'Sport',
            'senenews sport': 'Sport',
            
            # People/C√©l√©brit√©s
            'people': 'People',
            'c√©l√©brit√©s': 'People',
            'celebrites': 'People',
            'senenews people': 'People',
            
            # Soci√©t√©
            'soci√©t√©': 'Soci√©t√©',
            'societe': 'Soci√©t√©',
            'justice': 'Soci√©t√©',
            
            # International
            'international': 'International',
            'afrique': 'International',
            'afrique - actualit√© senenews people': 'International',
            
            # Multimedia/Divertissement
            'multimedia': 'Multimedia',
            'multim√©dia': 'Multimedia',
            'clip vid√©o': 'Multimedia',
            'senenews tv': 'Multimedia',
            
            # Contributions/Opinion
            'contributions': 'Opinion',
            'contribution': 'Opinion',
            
            # M√©t√©o
            'meteo': 'M√©t√©o',
            'm√©t√©o': 'M√©t√©o',
            
            # Premium/Sp√©cial
            'articles premium': 'Premium',
            'premium': 'Premium',
            
            # Live/Direct
            'en direct': 'Live',
            'live': 'Live',
            'en direct / live': 'Live'
        }
        
        # Cas sp√©ciaux pour les th√®mes commen√ßant par "S√©n√©gal - Actualit√©s >"
        if theme.startswith("S√©n√©gal - Actualit√©s"):
            # Cas sp√©ciaux pour "Actualit√©"
            if (theme == "S√©n√©gal - Actualit√©s" or 
                "A-La-Une" in theme or 
                "Notification" in theme):
                return "Actualit√©"
            
            # Extraire le terme apr√®s "S√©n√©gal - Actualit√©s >"
            if ">" in theme:
                parts = theme.split(">")
                if len(parts) >= 2:
                    main_theme = parts[1].strip()
                    # Si il y a encore des sous-cat√©gories, prendre la premi√®re
                    if ">" in main_theme:
                        main_theme = main_theme.split(">")[0].strip()
                    
                    # Appliquer le mapping
                    main_theme_lower = main_theme.lower()
                    if main_theme_lower in theme_mapping:
                        return theme_mapping[main_theme_lower]
                    else:
                        # Capitaliser proprement
                        return main_theme.capitalize()
        
        # V√©rifier le mapping direct
        if theme_lower in theme_mapping:
            return theme_mapping[theme_lower]
        
        # Recherche de mots-cl√©s dans le th√®me
        for keyword, mapped_theme in theme_mapping.items():
            if keyword in theme_lower:
                return mapped_theme
        
        # Cas par d√©faut
        if theme_lower in ['actualit√©s', 'actualites', 'actualit√©', 'actualite']:
            return "Actualit√©"
        
        # Si aucune r√®gle ne s'applique, capitaliser le premier mot
        first_word = theme.split()[0] if theme.split() else theme
        if len(first_word) > 2:
            return first_word.capitalize()
        
        return "Autre"
    
    def parse_french_date(self, date_str):
        """Convertit une date fran√ßaise en format datetime"""
        months_fr = {
            'janvier': 1, 'f√©vrier': 2, 'mars': 3, 'avril': 4, 'mai': 5, 'juin': 6,
            'juillet': 7, 'ao√ªt': 8, 'septembre': 9, 'octobre': 10, 'novembre': 11, 'd√©cembre': 12
        }
        
        try:
            date_str = date_str.strip().lower()
            pattern = r'(\d{1,2})\s+(\w+)\s+(\d{4})'
            match = re.search(pattern, date_str)
            
            if match:
                day = int(match.group(1))
                month_name = match.group(2)
                year = int(match.group(3))
                
                if month_name in months_fr:
                    month = months_fr[month_name]
                    return datetime(year, month, day)
        except Exception as e:
            print(f"‚ö† Erreur parsing date fran√ßaise '{date_str}': {e}")
        
        return None
    
    def parse_senenews_date(self, date_string):
        """Parse date from SeneNews format '12/06/2025 √† 13:05'"""
        try:
            date_match = re.search(r'(\d{2}/\d{2}/\d{4})\s+√†\s+(\d{2}:\d{2})', date_string)
            if date_match:
                date_part = date_match.group(1)
                time_part = date_match.group(2)
                return datetime.strptime(f"{date_part} {time_part}", "%d/%m/%Y %H:%M")
        except Exception as e:
            print(f"‚ö† Erreur parsing date SeneNews '{date_string}': {e}")
        return None
    
    def is_date_in_range(self, article_date, start_date, end_date):
        """V√©rifier si la date de l'article est dans l'intervalle sp√©cifi√©"""
        if not article_date:
            return False
        return start_date <= article_date <= end_date
    
    def get_soup(self, url):
        """R√©cup√©rer et parser une page web"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"‚ùå Erreur r√©cup√©ration {url}: {e}")
            return None
    
    def scrape_senenews(self, start_date, end_date, max_pages=10):
        """Scraper SeneNews"""
        print("\nüî• SCRAPING SENENEWS üî•")
        base_url = "https://www.senenews.com"
        articles_found = 0
        
        for page in range(1, max_pages + 1):
            print(f"\nüìÑ SeneNews - Page {page}")
            
            if page == 1:
                page_url = f"{base_url}/category/actualites"
            else:
                page_url = f"{base_url}/category/actualites/page/{page}"
            
            soup = self.get_soup(page_url)
            if not soup:
                continue
            
            # R√©cup√©rer les liens d'articles
            article_links = []
            selectors = [
                'h2 a[href*="senenews.com"]',
                'h3 a[href*="senenews.com"]',
                '.entry-title a',
                'article a[href*="senenews.com"]',
                'a[href*="/20"]'
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href and 'senenews.com' in href:
                        full_url = urljoin(base_url, href)
                        if full_url not in article_links:
                            article_links.append(full_url)
            
            if not article_links:
                print(f"‚ùå Aucun article trouv√© sur la page {page}")
                break
            
            print(f"üîç {len(article_links)} articles trouv√©s")
            
            page_articles_in_range = 0
            articles_too_old = 0
            
            for article_url in article_links:
                article_data = self.extract_senenews_article(article_url)
                
                if article_data and article_data['date']:
                    article_date = self.parse_senenews_date(article_data['date'])
                    
                    if article_date:
                        if self.is_date_in_range(article_date, start_date, end_date):
                            article_data['source'] = 'SeneNews'
                            article_data['theme_original'] = article_data.get('rubrique', 'Actualit√©s')
                            article_data['date_parsed'] = article_date.strftime('%Y-%m-%d %H:%M')
                            self.all_articles.append(article_data)
                            page_articles_in_range += 1
                            articles_found += 1
                            print(f"‚úÖ Article ajout√©: {article_data['titre'][:50]}...")
                        elif article_date < start_date:
                            articles_too_old += 1
                
                time.sleep(1)
            
            print(f"üìä Page {page}: {page_articles_in_range} articles dans la p√©riode")
            
            if articles_too_old > page_articles_in_range and page_articles_in_range == 0:
                print("üõë Articles trop anciens, arr√™t du scraping SeneNews")
                break
            
            time.sleep(2)
        
        print(f"‚úÖ SeneNews termin√©: {articles_found} articles r√©cup√©r√©s")
        return articles_found
    
    def extract_senenews_article(self, article_url):
        """Extraire les donn√©es d'un article SeneNews"""
        try:
            soup = self.get_soup(article_url)
            if not soup:
                return None
            
            data = {
                'url': article_url,
                'titre': '',
                'auteur': '',
                'date': '',
                'rubrique': '',
                'contenu': ''
            }
            
            # Titre
            title_elem = soup.find('h1', class_='entry-title')
            if title_elem:
                data['titre'] = title_elem.get_text(strip=True)
            
            # Auteur
            author_elem = soup.find('a', class_='aSingle')
            if author_elem:
                data['auteur'] = author_elem.get_text(strip=True)
            
            # Date
            time_elem = soup.find('time')
            if time_elem:
                date_span = time_elem.find('span', class_='date updated')
                if date_span:
                    data['date'] = date_span.get_text(strip=True)
            
            # Rubrique
            breadcrumb = []
            nav_links = soup.select('p a[href*="category"]')
            for link in nav_links:
                breadcrumb.append(link.get_text(strip=True))
            data['rubrique'] = ' > '.join(breadcrumb) if breadcrumb else 'Actualit√©s'
            
            # Contenu
            content_paragraphs = []
            content_area = soup.find('div', id='articleBody') or soup.find('div', class_='content-single-full')
            
            if content_area:
                paragraphs = content_area.find_all('p')
                for p in paragraphs:
                    if p.find_parent('div', class_='responsiveinpost'):
                        continue
                    if p.find_parent('ins', class_='adsbygoogle'):
                        continue
                    
                    text = p.get_text(strip=True)
                    if (text and len(text) > 30 and 
                        not any(skip in text.lower() for skip in ['partager', 'suivez', 'lire aussi', 'tags:', 'par ', 'source:', 'facebook', 'twitter', 'whatsapp', 'advertisement'])):
                        content_paragraphs.append(text)
            
            data['contenu'] = '\n\n'.join(content_paragraphs)
            return data
            
        except Exception as e:
            print(f"‚ùå Erreur extraction SeneNews {article_url}: {e}")
            return None
    
    def scrape_senego(self, start_date, end_date, max_pages=10):
        """Scraper Senego"""
        print("\nüåü SCRAPING SENEGO üåü")
        base_url = "https://senego.com"
        articles_found = 0
        
        # R√©cup√©rer les th√®mes
        try:
            soup = self.get_soup(base_url)
            if not soup:
                return 0
            
            menu_items = soup.select("header nav.nav .top-menu-content-wrapper .menuItemWrapper a.navItem")
            navigation = [{'theme': a.text.strip(), 'url': base_url + a['href'] if a['href'].startswith('/') else a['href']}
                         for a in menu_items]
            themes_dict = {item['theme'].lower(): item['url'] for item in navigation if '/rubrique/' in item['url']}
            
            if themes_dict:
                first_key = next(iter(themes_dict))
                themes_dict.pop(first_key)
                
        except Exception as e:
            print(f"‚ùå Erreur r√©cup√©ration menu Senego: {e}")
            return 0
        
        # Scraper chaque th√®me
        for theme, base_link in themes_dict.items():
            print(f"\nüìö Senego - Th√®me: {theme}")
            theme_articles_found = 0
            should_continue_theme = True
            
            for page_num in range(1, max_pages + 1):
                if not should_continue_theme:
                    break
                    
                url = f"{base_link}/page/{page_num}" if page_num > 1 else base_link
                print(f"üìÑ Page {page_num}")
                
                soup = self.get_soup(url)
                if not soup:
                    break
                
                articles = soup.select("section.sectionWithSidebar section.postsSectionCenter article")
                if not articles:
                    break
                
                page_articles_in_range = 0
                page_articles_too_old = 0
                
                for article in articles:
                    try:
                        title_tag = article.select_one("h2.archive-post-title a")
                        if not title_tag:
                            continue
                            
                        titre = title_tag.get_text(strip=True)
                        article_url = title_tag['href']
                        
                        auteur_elem = article.select_one("span.archive-post-author")
                        date_elem = article.select_one("span.archive-post-date")
                        
                        auteur = auteur_elem.get_text(strip=True) if auteur_elem else "Auteur inconnu"
                        date_str = date_elem.get_text(strip=True) if date_elem else "Date inconnue"
                        
                        # Parser la date
                        article_date = self.parse_french_date(date_str)
                        
                        # V√©rifier si dans la p√©riode
                        if article_date:
                            if self.is_date_in_range(article_date, start_date, end_date):
                                # R√©cup√©rer le contenu
                                article_soup = self.get_soup(article_url)
                                if article_soup:
                                    content_tag = article_soup.select_one("div.articleLeftContainer article div.article-detail-content123")
                                    contenu = content_tag.get_text(separator="\n", strip=True) if content_tag else "Contenu vide"
                                    
                                    self.all_articles.append({
                                        "source": "Senego",
                                        "theme_original": theme,
                                        "titre": titre,
                                        "date": date_str,
                                        "date_parsed": article_date.strftime('%Y-%m-%d'),
                                        "auteur": auteur,
                                        "contenu": contenu,
                                        "url": article_url,
                                        "rubrique": theme
                                    })
                                    
                                    page_articles_in_range += 1
                                    theme_articles_found += 1
                                    articles_found += 1
                                    print(f"‚úÖ Article ajout√©: {titre[:50]}...")
                            elif article_date < start_date:
                                page_articles_too_old += 1
                        
                        time.sleep(0.5)
                        
                    except Exception as e:
                        print(f"‚ùå Erreur article Senego: {e}")
                
                print(f"üìä Page {page_num}: {page_articles_in_range} articles dans la p√©riode")
                
                # Arr√™ter si tous les articles sont trop anciens
                if page_articles_too_old > 0 and page_articles_in_range == 0:
                    print(f"üõë Arr√™t th√®me {theme}: articles trop anciens")
                    should_continue_theme = False
                
                time.sleep(1)
            
            print(f"üìä Th√®me {theme}: {theme_articles_found} articles")
        
        print(f"‚úÖ Senego termin√©: {articles_found} articles r√©cup√©r√©s")
        return articles_found
    
    def process_themes(self):
        """Traite et harmonise tous les th√®mes apr√®s collecte"""
        print("\nüîÑ HARMONISATION DES TH√àMES")
        
        if not self.all_articles:
            print("‚ùå Aucun article √† traiter")
            return
        
        # Cr√©er le DataFrame
        df = pd.DataFrame(self.all_articles)
        
        # Afficher les th√®mes originaux pour debug
        print("\nüìä Th√®mes originaux trouv√©s:")
        if 'theme_original' in df.columns:
            theme_counts = df['theme_original'].value_counts()
            for theme, count in theme_counts.items():
                print(f"   ‚Ä¢ {theme}: {count} articles")
        
        # Harmoniser les th√®mes
        if 'theme_original' in df.columns:
            df['theme'] = df['theme_original'].apply(self.harmonize_theme)
        elif 'theme' in df.columns:
            df['theme'] = df['theme'].apply(self.harmonize_theme)
        else:
            df['theme'] = 'Autre'
        
        # Afficher les th√®mes harmonis√©s
        print("\n‚ú® Th√®mes harmonis√©s:")
        theme_counts_harmonized = df['theme'].value_counts()
        for theme, count in theme_counts_harmonized.items():
            print(f"   ‚Ä¢ {theme}: {count} articles")
        
        # Supprimer les colonnes temporaires
        columns_to_remove = ['theme_original', 'rubrique']
        for col in columns_to_remove:
            if col in df.columns:
                df = df.drop(columns=[col])
        
        # Mettre √† jour all_articles
        self.all_articles = df.to_dict('records')
        
        print(f"‚úÖ Harmonisation termin√©e: {len(df['theme'].unique())} th√®mes uniques")
    
    def scrape_all(self, days_back=1, max_pages=10):
        """Scraper les deux sites"""
        print(f"üöÄ SCRAPER UNIFI√â - R√©cup√©ration des {days_back} derniers jours")
        
        # Calculer les dates
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        print(f"üìÖ P√©riode: {start_date.strftime('%d/%m/%Y %H:%M')} - {end_date.strftime('%d/%m/%Y %H:%M')}")
        
        # Scraper les deux sites
        senenews_count = self.scrape_senenews(start_date, end_date, max_pages)
        senego_count = self.scrape_senego(start_date, end_date, max_pages)
        
        # Traiter les th√®mes apr√®s collecte
        self.process_themes()
        
        total_articles = len(self.all_articles)
        
        print(f"\nüéâ R√âSUM√â FINAL:")
        print(f"   üì∞ SeneNews: {senenews_count} articles")
        print(f"   üì∞ Senego: {senego_count} articles")
        print(f"   üì∞ Total: {total_articles} articles")
        
        return self.all_articles
    
    def save_to_csv(self, filename=None):
        """Sauvegarder tous les articles dans un fichier CSV unifi√©"""
        if not self.all_articles:
            print("‚ùå Aucun article √† sauvegarder")
            return None
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"articles_unifies_{timestamp}.csv"
        
        # Cr√©er le DataFrame
        df = pd.DataFrame(self.all_articles)
        
        # Trier par date si possible
        if 'date_parsed' in df.columns:
            df = df.sort_values('date_parsed', ascending=False)
        
        # R√©organiser les colonnes
        columns_order = ['source', 'theme', 'titre', 'date', 'date_parsed', 'auteur', 'contenu', 'url']
        existing_columns = [col for col in columns_order if col in df.columns]
        df = df[existing_columns]
        
        # Sauvegarder
        df.to_csv(filename, index=False, encoding='utf-8')
        
        print(f"üíæ Fichier CSV sauvegard√©: {filename}")
        print(f"üìä Colonnes: {list(df.columns)}")
        print(f"üìà Lignes: {len(df)}")
        
        # Afficher la r√©partition finale des th√®mes
        print(f"\nüìä R√©partition finale des th√®mes:")
        theme_distribution = df['theme'].value_counts()
        for theme, count in theme_distribution.items():
            percentage = (count / len(df)) * 100
            print(f"   ‚Ä¢ {theme}: {count} articles ({percentage:.1f}%)")
        
        return filename

def main():
    scraper = UnifiedNewsScraper()
    
    try:
        # Scraper les deux sites
        articles = scraper.scrape_all(days_back=1, max_pages=15)
        
        if articles:
            # Sauvegarder le fichier CSV unifi√©
            csv_file = scraper.save_to_csv()
            
            print(f"\nüéØ SCRAPING TERMIN√â AVEC SUCC√àS!")
            print(f"üìÑ Fichier CSV unifi√©: {csv_file}")
            print(f"üîó Sources scrap√©es: SeneNews + Senego")
            print(f"üè∑Ô∏è Th√®mes harmonis√©s automatiquement")
            
        else:
            print("\n‚ùå Aucun article trouv√© dans la p√©riode sp√©cifi√©e")
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Scraping interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur pendant le scraping: {e}")

if __name__ == "__main__":
    main()